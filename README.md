# TCLM2: Transformer-based Causal Language Model v2

**TCLM2** is a custom, decoder-only Transformer model trained using **causal language modeling (CLM)**. It is designed for autoregressive text generation tasks and supports fine-tuning and inference workflows similar to GPT-style models.

---

## ğŸ§  Overview

TCLM2 is built for fast, scalable, and high-quality text generation. It uses multi-layer self-attention with positional embeddings, LayerNorm, and residual connections, trained on a large corpus with left-to-right prediction objectives.

---

## ğŸš€ Features

- âš¡ Decoder-only Transformer with custom architecture
- ğŸ“– Causal (autoregressive) language modeling
- ğŸ”§ Hugging Face-compatible interface for loading, fine-tuning, and inference
- ğŸ”¬ Pre-trained using `Trainer` with dynamic masking and gradient checkpointing

---

## ğŸ“ Project Structure

```bash
TCLM2/
â”œâ”€â”€ model/                # Model architecture (config, forward, init)
â”œâ”€â”€ tokenizer/            # Tokenizer setup and special tokens
â”œâ”€â”€ training/             # Scripts for training and evaluation
â”œâ”€â”€ generate.py           # Script for text generation
â”œâ”€â”€ config.json           # Model configuration
â”œâ”€â”€ pytorch_model.bin     # Model weights (optional)
â””â”€â”€ README.md             # Documentation
