{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc2SvHlgLP_J"
      },
      "source": [
        "###Connect to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rk5VitodxQv_",
        "outputId": "7f462b05-c1ee-465f-bba0-b18764d4a37b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgrYC2g_xDBJ"
      },
      "source": [
        "###configuration file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "djwQ4krXwH0M"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def get_config():\n",
        "    return {\n",
        "        \"logs\": \"/content/drive/MyDrive/Colab Notebooks/T-CLM/T-CLM_log\",\n",
        "        \"batch_size\": 8,\n",
        "        \"num_epochs\": 50,\n",
        "        \"lr\": 1e-4,\n",
        "        \"seq_len\": 512,\n",
        "        \"d_model\": 768,\n",
        "        \"n_layers\": 12,\n",
        "        \"head\": 12,\n",
        "        \"d_ff\": 3072,\n",
        "        \"dropout\": 0.1,\n",
        "        \"masking_prob\": 0.15,\n",
        "        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "        \"model_file_path\": \"/content/drive/MyDrive/Colab Notebooks/T-CLM/T-CLM2.pt\",\n",
        "        \"tokenizer_file\": \"/content/drive/MyDrive/Colab Notebooks/T-CLM/tokenizer.json\",\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2a7fTMsxGUH"
      },
      "source": [
        "###BPE Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yv9S_PQcwj0i"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
        "\n",
        "def get_all_sentences(ds, field):\n",
        "    for item in ds:\n",
        "        yield item[field]\n",
        "\n",
        "def build_or_get_tokenizer(config, ds):\n",
        "    tokenizer_path = Path(config['tokenizer_file'])\n",
        "    if not tokenizer_path.exists():\n",
        "        tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
        "        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "        trainer = trainers.BpeTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=1)\n",
        "        tokenizer.train_from_iterator(get_all_sentences(ds, \"text\"), trainer=trainer)\n",
        "        tokenizer.save(str(tokenizer_path))\n",
        "    else:\n",
        "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
        "\n",
        "    return tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1LIieN_LbB5"
      },
      "source": [
        "###Data Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1VPhoImUwpcQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import json\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class BilingualDataset(Dataset):\n",
        "    def __init__(self, ds, tokenizer, seq_len):\n",
        "        self.seq_len = seq_len\n",
        "        self.ds = ds\n",
        "        self.tokenizer = tokenizer\n",
        "        self.sos_token = torch.tensor([tokenizer.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
        "        self.eos_token = torch.tensor([tokenizer.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
        "        self.pad_token = torch.tensor([tokenizer.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.ds[idx]['text']\n",
        "        input_tokens = self.tokenizer.encode(text).ids\n",
        "\n",
        "        # Truncate if too long\n",
        "        if len(input_tokens) > self.seq_len - 2:\n",
        "            input_tokens = input_tokens[:self.seq_len - 2]\n",
        "\n",
        "        num_padding_tokens = self.seq_len - len(input_tokens) - 2\n",
        "\n",
        "        input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # The label is shifted right by one\n",
        "        label = torch.cat(\n",
        "            [\n",
        "                torch.tensor(input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * (num_padding_tokens + 1), dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        assert input.size(0) == self.seq_len\n",
        "        assert label.size(0) == self.seq_len\n",
        "\n",
        "        return {\n",
        "            \"input\": input,\n",
        "            \"label\": label,\n",
        "            \"text\": text,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFjxO_cGMVg5"
      },
      "source": [
        "###Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MQyv27OIMG5J"
      },
      "outputs": [],
      "source": [
        "def get_ds(config):\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/T-CLM/unlabeled_dataset.json', 'r', encoding='utf-8') as f:\n",
        "        ds_raw = json.load(f)\n",
        "\n",
        "    # ds_raw = load_dataset(f\"bookcorpus/bookcorpus\", f\"plain_text\", split='train', trust_remote_code=True)\n",
        "\n",
        "    tokenizer = build_or_get_tokenizer(config, ds_raw)\n",
        "    train_ds_size = int(0.99 * len(ds_raw))\n",
        "    val_ds_size = len(ds_raw) - train_ds_size\n",
        "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
        "\n",
        "    train_ds = BilingualDataset(train_ds_raw, tokenizer, config['seq_len'])\n",
        "    val_ds = BilingualDataset(val_ds_raw, tokenizer, config['seq_len'])\n",
        "\n",
        "    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
        "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
        "\n",
        "    return train_dataloader, val_dataloader, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI2RCCppxLNv"
      },
      "source": [
        "###Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ycm0_YdhwnJL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.alpha = nn.Parameter(torch.ones(features))\n",
        "        self.bias = nn.Parameter(torch.zeros(features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
        "\n",
        "class FeedForwardBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
        "\n",
        "class InputEmbeddings(nn.Module):\n",
        "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class ResidualConnection(nn.Module):\n",
        "    def __init__(self, features: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.h = h\n",
        "        assert d_model % h == 0, \"d_model must be divisible by h\"\n",
        "        self.d_k = d_model // h\n",
        "\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "        d_k = query.shape[-1]\n",
        "        scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        scores = F.softmax(scores, dim=-1)\n",
        "        if dropout is not None:\n",
        "            scores = dropout(scores)\n",
        "        return scores @ value, scores\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        query = self.w_q(q).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
        "        key = self.w_k(k).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
        "        value = self.w_v(v).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "        x, attn = self.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
        "        return self.w_o(x)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
        "\n",
        "    def forward(self, x, tgt_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
        "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, layers: nn.ModuleList, norm_layer: LayerNormalization) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = norm_layer\n",
        "\n",
        "    def forward(self, x, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, tgt_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class ProjectionLayer(nn.Module):\n",
        "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.proj(x)\n",
        "\n",
        "class TCLM(nn.Module):\n",
        "    def __init__(self, vocab_size: int, seq_len: int, d_model: int, N: int, h: int, dropout: float, d_ff: int):\n",
        "        super().__init__()\n",
        "        self.input_embed = InputEmbeddings(d_model, vocab_size)\n",
        "        self.pos_embed = PositionalEncoding(d_model, seq_len, dropout)\n",
        "\n",
        "        # Decoder blocks with multi-head attention and feed-forward\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderBlock(\n",
        "                features=d_model,\n",
        "                self_attention_block=MultiHeadAttentionBlock(d_model, h, dropout),\n",
        "                feed_forward_block=FeedForwardBlock(d_model, d_ff, dropout),\n",
        "                dropout=dropout\n",
        "            ) for _ in range(N)\n",
        "        ])\n",
        "\n",
        "        self.decoder = Decoder(self.layers, LayerNormalization(d_model))\n",
        "        self.projection_layer = ProjectionLayer(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        token_embed = self.input_embed(idx)\n",
        "        x = self.pos_embed(token_embed)\n",
        "\n",
        "        tgt_mask = torch.tril(torch.ones((T, T), device=idx.device)).unsqueeze(0).unsqueeze(0)\n",
        "        x = self.decoder(x, tgt_mask)\n",
        "\n",
        "        logits = self.projection_layer(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            logits = logits[:, :-1, :].contiguous()\n",
        "            targets = targets[:, 1:].contiguous()\n",
        "            logits = logits.view(-1, logits.size(-1))\n",
        "            targets = targets.view(-1)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        else:\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx: torch.Tensor, max_new_tokens: int, seq_len: int):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_crop = idx[:, -seq_len:]\n",
        "            logits, _ = self.forward(idx_crop)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6Z-sKhGvQSc"
      },
      "source": [
        "###Preload model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sjl15GUkvS8m"
      },
      "outputs": [],
      "source": [
        "def load_model(config, device, model, tokenizer, optimizer):\n",
        "    initial_epoch = 0\n",
        "    global_step = 0\n",
        "    model = model.to(device)\n",
        "    model_file_path = config['model_file_path']\n",
        "    if Path(model_file_path).exists():\n",
        "        print(f'Loading model from {str(model_file_path)}')\n",
        "        state = torch.load(str(model_file_path), map_location=device, weights_only=True)\n",
        "        model.load_state_dict(state['model_state_dict'])\n",
        "        initial_epoch = state['epoch'] + 1\n",
        "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "        global_step = state['global_step']\n",
        "    else:\n",
        "        print(\"No model file found.\")\n",
        "\n",
        "    return model, initial_epoch, global_step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHEZMF_edPIc"
      },
      "source": [
        "###WANDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oR1KEZ50ZGuW"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Em-UtNUhWRl9"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPOUVLnjKz2-"
      },
      "source": [
        "###Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wmm5oPx4wr3U"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import warnings\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "\n",
        "def train(config):\n",
        "    # Initialize wandb\n",
        "    wandb.init(project=\"T-CLM2\", config=config)\n",
        "\n",
        "    device = config['device']\n",
        "    print(\"Using device:\", device)\n",
        "    if device == 'cuda':\n",
        "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
        "        print(f\"Device memory: {round(torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3, 1)} GB\")\n",
        "    device = torch.device(device)\n",
        "\n",
        "    train_dataloader, val_dataloader, tokenizer = get_ds(config)\n",
        "    model = TCLM(vocab_size=tokenizer.get_vocab_size(), seq_len=config['seq_len'], d_model=config['d_model'], N=config['n_layers'], h=config['head'], dropout=config['dropout'], d_ff=config['d_ff'])\n",
        "\n",
        "    # Log model configuration in wandb\n",
        "    wandb.watch(model, log=\"all\")\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
        "    model, initial_epoch, global_step = load_model(config, device, model, tokenizer, optimizer)\n",
        "\n",
        "    for epoch in range(initial_epoch, config['num_epochs']):\n",
        "        torch.cuda.empty_cache()\n",
        "        model.train()\n",
        "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
        "        total_loss = 0\n",
        "        num_batches = len(train_dataloader)\n",
        "\n",
        "        for batch in batch_iterator:\n",
        "            encoder_input = batch['input'].to(device)\n",
        "            targets = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()  # Reset gradients\n",
        "            logits, loss = model(encoder_input, targets=targets)\n",
        "\n",
        "            total_loss += loss.item()  # Accumulate loss\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
        "            optimizer.step()\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "            # Log batch loss to wandb\n",
        "            wandb.log({\"batch_loss\": loss.item(), \"global_step\": global_step})\n",
        "\n",
        "            batch_iterator.set_postfix({'Loss': loss.item()})\n",
        "\n",
        "        # Epoch-level logging\n",
        "        avg_loss = total_loss / num_batches\n",
        "        print(f\"Epoch {epoch} | Avg Loss: {round(avg_loss, 2)}\")\n",
        "\n",
        "        # Log average loss for epoch to wandb\n",
        "        wandb.log({\"average_loss\": avg_loss, \"epoch\": epoch})\n",
        "\n",
        "        # Model checkpointing\n",
        "        model_filename = f\"/content/drive/MyDrive/Colab Notebooks/T-CLM/t-CLM2.pt\"\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'global_step': global_step,\n",
        "        }, model_filename)\n",
        "\n",
        "        validate(model, val_dataloader, device, epoch)\n",
        "\n",
        "def validate(model, val_dataloader, device, epoch):\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    num_batches = len(val_dataloader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            encoder_input = batch['input'].to(device)\n",
        "            targets = batch['label'].to(device)\n",
        "            logits, val_loss = model(encoder_input, targets=targets)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / num_batches\n",
        "    print(f\"Validation Loss (Epoch {epoch}): {avg_val_loss}\")\n",
        "\n",
        "    # Log validation loss to wandb\n",
        "    wandb.log({\"validation_loss\": avg_val_loss, \"epoch\": epoch})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    config = get_config()\n",
        "    train(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpbUaKraacpj",
        "outputId": "b29b2d0b-6270-402d-e07a-56e10243b618"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from /content/drive/MyDrive/Colab Notebooks/T-CLM/T-CLM2.pt\n",
            "text:  I went to \n",
            "predicted: I went to ES our Sem0 Thatcher Make diceEilled\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "config = get_config()\n",
        "_, _, tokenizer = get_ds(config)\n",
        "\n",
        "# Create the model\n",
        "model = TCLM(vocab_size=tokenizer.get_vocab_size(),\n",
        "             seq_len=config['seq_len'],\n",
        "             d_model=config['d_model'],\n",
        "             N=config['n_layers'],\n",
        "             h=config['head'],\n",
        "             dropout=config['dropout'],\n",
        "             d_ff=config['d_ff'])\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
        "\n",
        "# Load model, epoch, and step\n",
        "model, initial_epoch, global_step = load_model(config, config['device'], model, tokenizer, optimizer)\n",
        "\n",
        "# Input text to generate a sequence\n",
        "text = \"I went to \"\n",
        "\n",
        "# Tokenize the input text\n",
        "idx = tokenizer.encode(text).ids\n",
        "\n",
        "# Convert idx to a tensor and add batch dimension\n",
        "idx = torch.tensor([idx]).to(config['device'])\n",
        "\n",
        "# Generate a sequence of tokens\n",
        "generated_sequence = model.generate(idx, max_new_tokens=100, seq_len=config['seq_len'])\n",
        "\n",
        "# Decode the generated sequence into text\n",
        "predicted_text = tokenizer.decode(generated_sequence[0].cpu().numpy())\n",
        "\n",
        "# Print the predicted text\n",
        "print(\"text: \", text)\n",
        "print(\"predicted:\", predicted_text)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
