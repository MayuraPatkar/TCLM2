{"cells":[{"cell_type":"markdown","metadata":{"id":"Sc2SvHlgLP_J"},"source":["###Connect to Drive"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rk5VitodxQv_","outputId":"5f5be537-7d38-48b7-fbc1-0de90c37633a","executionInfo":{"status":"ok","timestamp":1726910637905,"user_tz":-330,"elapsed":4694,"user":{"displayName":"Mayura","userId":"14003289813185148827"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"BgrYC2g_xDBJ"},"source":["###configuration file"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"djwQ4krXwH0M","executionInfo":{"status":"ok","timestamp":1726910637905,"user_tz":-330,"elapsed":4,"user":{"displayName":"Mayura","userId":"14003289813185148827"}}},"outputs":[],"source":["def get_config():\n","    return {\n","        \"model\":\"T-CLM\",\n","        \"logs\": \"/content/drive/MyDrive/Colab Notebooks/T-CLM/T-CLM_logs\",\n","        \"batch_size\": 8,\n","        \"num_epochs\": 30,\n","        \"lr\": 1e-4,\n","        \"seq_len\": 400,\n","        \"d_model\": 512,\n","        \"n_layers\": 6,\n","        \"head\": 8,\n","        \"d_ff\": 2048,\n","        \"dropout\": 0.1,\n","        \"masking_prob\": 0.15,\n","        \"vocab_size\": 13246,\n","        \"model_file_path\": \"/content/drive/MyDrive/Colab Notebooks/T-CLM/T-CLM.pt\",\n","        \"tokenizer_file\": \"/content/drive/MyDrive/Colab Notebooks/T-CLM/tokenizer.json\",\n","    }\n"]},{"cell_type":"markdown","metadata":{"id":"g2a7fTMsxGUH"},"source":["###BPE Tokenizer\n"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"yv9S_PQcwj0i","executionInfo":{"status":"ok","timestamp":1726910639412,"user_tz":-330,"elapsed":7,"user":{"displayName":"Mayura","userId":"14003289813185148827"}}},"outputs":[],"source":["from pathlib import Path\n","from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n","\n","def get_all_sentences(ds, field):\n","    for item in ds:\n","        yield item[field]\n","\n","def build_or_get_tokenizer(config, ds):\n","    tokenizer_path = Path(config['tokenizer_file'])\n","    if not tokenizer_path.exists():\n","        tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n","        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n","        trainer = trainers.BpeTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\", \"[MASK]\"], min_frequency=1)\n","        get_all_sentences(ds,'text')\n","        tokenizer.train_from_iterator(get_all_sentences(ds, \"text\"), trainer=trainer)\n","        tokenizer.save(str(tokenizer_path))\n","    else:\n","        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n","\n","    return tokenizer"]},{"cell_type":"markdown","metadata":{"id":"o1LIieN_LbB5"},"source":["###Data Pipeline"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"1VPhoImUwpcQ","executionInfo":{"status":"ok","timestamp":1726910641970,"user_tz":-330,"elapsed":6,"user":{"displayName":"Mayura","userId":"14003289813185148827"}}},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader, random_split\n","import json\n","from torch.utils.data import Dataset\n","\n","class BilingualDataset(Dataset):\n","    def __init__(self, ds, tokenizer, seq_len, num_heads):\n","        self.seq_len = seq_len\n","        self.ds = ds\n","        self.tokenizer = tokenizer\n","        self.num_heads = num_heads\n","        self.sos_token = torch.tensor([tokenizer.token_to_id(\"[SOS]\")], dtype=torch.int64)\n","        self.eos_token = torch.tensor([tokenizer.token_to_id(\"[EOS]\")], dtype=torch.int64)\n","        self.pad_token = torch.tensor([tokenizer.token_to_id(\"[PAD]\")], dtype=torch.int64)\n","\n","    def __len__(self):\n","        return len(self.ds)\n","\n","    def __getitem__(self, idx):\n","        text = self.ds[idx]['text']\n","        input_tokens = self.tokenizer.encode(text).ids\n","\n","        # Truncate if too long\n","        if len(input_tokens) > self.seq_len - 2:\n","            input_tokens = input_tokens[:self.seq_len - 2]\n","\n","        num_padding_tokens = self.seq_len - len(input_tokens) - 2\n","\n","        input = torch.cat(\n","            [\n","                self.sos_token,\n","                torch.tensor(input_tokens, dtype=torch.int64),\n","                self.eos_token,\n","                torch.tensor([self.pad_token] * num_padding_tokens, dtype=torch.int64),\n","            ],\n","            dim=0,\n","        )\n","\n","        # The label is shifted right by one\n","        label = torch.cat(\n","            [\n","                torch.tensor(input_tokens, dtype=torch.int64),\n","                self.eos_token,\n","                torch.tensor([self.pad_token] * (num_padding_tokens + 1), dtype=torch.int64),\n","            ],\n","            dim=0,\n","        )\n","\n","        assert input.size(0) == self.seq_len\n","        assert label.size(0) == self.seq_len\n","\n","        return {\n","            \"input\": input,\n","            \"label\": label,\n","            \"mask\": self.create_mask(input.size(0)),\n","            \"text\": text,\n","        }\n","\n","    def create_mask(self, size):\n","      mask = torch.triu(torch.ones((size, size)), diagonal=1)\n","      mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","      return mask.unsqueeze(0).expand(self.num_heads, size, size)"]},{"cell_type":"markdown","metadata":{"id":"oFjxO_cGMVg5"},"source":["###Load dataset"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"MQyv27OIMG5J","executionInfo":{"status":"ok","timestamp":1726910653711,"user_tz":-330,"elapsed":651,"user":{"displayName":"Mayura","userId":"14003289813185148827"}}},"outputs":[],"source":["def get_ds(config):\n","    with open('/content/drive/MyDrive/Colab Notebooks/T-CLM/dataset.json', 'r', encoding='utf-8') as f:\n","        ds_raw = json.load(f)\n","\n","    # ds_raw = load_dataset(f\"bookcorpus/bookcorpus\", f\"plain_text\", split='train', trust_remote_code=True)\n","\n","    tokenizer = build_or_get_tokenizer(config, ds_raw)\n","    train_ds_size = int(0.99 * len(ds_raw))\n","    val_ds_size = len(ds_raw) - train_ds_size\n","    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n","\n","    train_ds = BilingualDataset(train_ds_raw, tokenizer, config['seq_len'], config['head'])\n","    val_ds = BilingualDataset(val_ds_raw, tokenizer, config['seq_len'], config['head'])\n","\n","    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n","    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n","\n","    return train_dataloader, val_dataloader, tokenizer"]},{"cell_type":"markdown","metadata":{"id":"QI2RCCppxLNv"},"source":["###Transformer model"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"ycm0_YdhwnJL","executionInfo":{"status":"ok","timestamp":1726910655411,"user_tz":-330,"elapsed":5,"user":{"displayName":"Mayura","userId":"14003289813185148827"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class DecoderLayer(nn.Module):\n","    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):\n","        super(DecoderLayer, self).__init__()\n","        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n","        self.linear1 = nn.Linear(d_model, dim_feedforward)\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear2 = nn.Linear(dim_feedforward, d_model)\n","\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","\n","        self.dropout1 = nn.Dropout(dropout)\n","        self.dropout2 = nn.Dropout(dropout)\n","\n","    def forward(self, tgt, tgt_mask=None):\n","        tgt2, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask)\n","        tgt = tgt + self.dropout1(tgt2)\n","        tgt = self.norm1(tgt)\n","\n","        tgt2 = self.linear2(self.dropout(F.relu(self.linear1(tgt))))\n","        tgt = tgt + self.dropout2(tgt2)\n","        tgt = self.norm2(tgt)\n","\n","        return tgt\n","\n","class TransformerDecoderOnly(nn.Module):\n","    def __init__(self, vocab_size, d_model: int, nhead: int, num_layers: int, dim_feedforward: int, max_len: int, dropout: int):\n","        super(TransformerDecoderOnly, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, d_model)\n","        self.pos_embedding = nn.Embedding(max_len, d_model)\n","\n","        self.layers = nn.ModuleList([DecoderLayer(d_model, nhead, dim_feedforward, dropout) for _ in range(num_layers)])\n","        self.linear = nn.Linear(d_model, vocab_size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # Initialize weights\n","        self._init_weights()\n","\n","    def _init_weights(self):\n","        nn.init.xavier_uniform_(self.embedding.weight)\n","        nn.init.xavier_uniform_(self.pos_embedding.weight)\n","        nn.init.xavier_uniform_(self.linear.weight)\n","        for layer in self.layers:\n","            nn.init.xavier_uniform_(layer.self_attn.in_proj_weight)\n","            nn.init.xavier_uniform_(layer.linear1.weight)\n","            nn.init.xavier_uniform_(layer.linear2.weight)\n","\n","    def forward(self, tgt, tgt_mask=None):\n","        batch_size, seq_len = tgt.size(0), tgt.size(1)\n","        pos = torch.arange(0, seq_len, dtype=torch.long, device=tgt.device).unsqueeze(0).expand(batch_size, seq_len)\n","        tgt = self.embedding(tgt) + self.pos_embedding(pos)\n","\n","        for layer in self.layers:\n","            tgt = layer(tgt, tgt_mask=tgt_mask)\n","\n","        logits = self.linear(tgt)\n","        return logits\n"]},{"cell_type":"code","source":["config = get_config()\n","model = TransformerDecoderOnly(config['vocab_size'], config['d_model'], config['head'], config['n_layers'], config['d_ff'], config['seq_len'], config['dropout'])\n","print(model)"],"metadata":{"id":"T-1nxa2dW0Iy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WjUFRoFrLCDz"},"source":["###Validation run"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"gpRG2ZWdKq3U","executionInfo":{"status":"ok","timestamp":1726910658869,"user_tz":-330,"elapsed":717,"user":{"displayName":"Mayura","userId":"14003289813185148827"}}},"outputs":[],"source":["import math\n","import torch\n","import torch.nn as nn\n","\n","def run_validation(model, val_dataloader, tokenizer, seq_len, device, log_fn, global_step, writer):\n","    model.eval()\n","    total_loss = 0.0\n","    num_batches = len(val_dataloader)\n","    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n","\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids = batch['input'].to(device)\n","            labels = batch['label'].to(device)\n","\n","            # Create causal mask\n","            seq_len = input_ids.size(1)\n","            batch_size = input_ids.size(0)\n","\n","            causal_mask = torch.tril(torch.ones((seq_len, seq_len), device=device)).unsqueeze(0)  # Shape: (1, seq_len, seq_len)\n","            causal_mask = causal_mask.unsqueeze(1).expand(batch_size, config['head'], config['seq_len'], config['seq_len'])  # Shape: (batch_size, n_heads, seq_len, seq_len)\n","            causal_mask = causal_mask.reshape(batch_size * config['head'], config['seq_len'], config['seq_len'])  # Shape: (batch_size * n_heads, seq_len, seq_len)\n","\n","            logits = model.forward(input_ids, causal_mask)\n","\n","            logits = logits.view(-1, logits.size(-1))\n","            labels = labels.view(-1)\n","\n","            loss = loss_fn(logits, labels)\n","            total_loss += loss.item()\n","\n","            import numpy as np\n","\n","            predictions = torch.argmax(logits, dim=-1)\n","            predicted_tokens = [tokenizer.id_to_token(int(pred)) for pred in predictions.cpu().numpy()]  # Convert to tokens\n","\n","    print(\"Raw predictions (indices):\", predictions.cpu().numpy())  # Print raw predicted token indices\n","    print(\"Text: \", batch['text'])\n","    print(\"Predicted: \", predicted_tokens)\n","    avg_loss = total_loss / num_batches\n","    perplexity = math.exp(avg_loss)\n","    print(f\"Validation | Avg Loss: {avg_loss:.4f} | Perplexity: {perplexity:.2f}\")\n","\n","    writer.add_scalar('Validation/Loss', avg_loss, global_step)\n","    writer.add_scalar('Validation/Perplexity', perplexity, global_step)\n","\n","    model.train()\n"]},{"cell_type":"markdown","metadata":{"id":"oFZOdFDDK7QL"},"source":["###Get model"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"h2w9W2hpKxQD","executionInfo":{"status":"ok","timestamp":1726910660591,"user_tz":-330,"elapsed":7,"user":{"displayName":"Mayura","userId":"14003289813185148827"}}},"outputs":[],"source":["def get_decoder_only_model(config, vocab_size):\n","    model = TransformerDecoderOnly(config['vocab_size'], config['d_model'], config['head'], config['n_layers'], config['d_ff'], config['seq_len'], config['dropout'])\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"K6Z-sKhGvQSc"},"source":["###Preload model"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"sjl15GUkvS8m","executionInfo":{"status":"ok","timestamp":1726910662394,"user_tz":-330,"elapsed":794,"user":{"displayName":"Mayura","userId":"14003289813185148827"}}},"outputs":[],"source":["def load_model(config, device, tokenizer, optimizer):\n","    if os.path.exists(config['model_file_path']):\n","        checkpoint = torch.load(config['model_file_path'], map_location=device)\n","        model = get_decoder_only_model(config, tokenizer.get_vocab_size()).to(device)\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        epoch = checkpoint['epoch']\n","        global_step = checkpoint.get('global_step', 0)\n","        print(f\"Loaded checkpoint from epoch {epoch}, global_step {global_step}\")\n","        return model, epoch, global_step\n","    else:\n","        print(\"No checkpoint found. Starting training from scratch.\")\n","        model = get_decoder_only_model(config, tokenizer.get_vocab_size()).to(device)\n","        return model, 0, 0\n"]},{"cell_type":"markdown","metadata":{"id":"sPOUVLnjKz2-"},"source":["###Training loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XzdqabrRx8GQ"},"outputs":[],"source":["!pip install torchmetrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wmm5oPx4wr3U"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, random_split\n","import warnings\n","import os\n","import sys\n","import json\n","from pathlib import Path\n","from tqdm import tqdm\n","from torch.utils.tensorboard import SummaryWriter\n","import torchmetrics\n","\n","def train(config):\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    print(\"Using device:\", device)\n","    if device == 'cuda':\n","        print(f\"Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n","        print(f\"Device memory: {torch.cuda.get_device_properties(device).total_memory / 1024 ** 3:.2f} GB\")\n","    device = torch.device(device)\n","\n","    train_dataloader, val_dataloader, tokenizer = get_ds(config)\n","    model = get_decoder_only_model(config, config['vocab_size']).to(device)\n","    writer = SummaryWriter(config['logs'])\n","    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n","    model, initial_epoch, global_step = load_model(config, device, tokenizer, optimizer).to(device)\n","    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n","\n","    for epoch in range(initial_epoch, config['num_epochs']):\n","        torch.cuda.empty_cache()\n","        model.train()\n","        batch_iterator = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{config['num_epochs']}\")\n","        total_loss = 0.0\n","        num_batches = len(train_dataloader)\n","\n","        for batch in batch_iterator:\n","            input_ids = batch['input'].to(device) # (batch_size, seq_len)\n","            labels = batch['label'].to(device)  # (batch_size, seq_len)\n","            causal_mask = batch['mask'].to(device) # (1, seq_len, seq_len)\n","            print(\"Input IDs shape:\", batch['input'].shape)\n","            print(\"Labels shape:\", batch['label'].shape)\n","            print(\"Causal Mask shape:\", batch['mask'].shape)\n","            sys.exit(0)\n","\n","            optimizer.zero_grad(set_to_none=True)\n","\n","            # Forward pass through the decoder-only model\n","            logits = model.forward(input_ids, causal_mask)  # (batch_size, seq_len, vocab_size)\n","\n","            # Reshape logits and labels for computing loss\n","            logits = logits.view(-1, logits.size(-1))      # (batch_size * seq_len, vocab_size)\n","            labels = labels.view(-1)                       # (batch_size * seq_len)\n","\n","            loss = loss_fn(logits, labels)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","            global_step += 1\n","            writer.add_scalar('Train/Loss', loss.item(), global_step)\n","            batch_iterator.set_postfix({'Loss': loss.item()})\n","\n","        # End of epoch logging\n","        avg_loss = total_loss / num_batches\n","        writer.add_scalar('Train/Average_Loss', avg_loss, epoch + 1)\n","\n","        # Validation\n","        run_validation(model, val_dataloader, tokenizer, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n","\n","        # Save the model checkpoint\n","        torch.save({\n","            'epoch': epoch + 1,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'global_step': global_step,\n","        }, config['model_file_path'])\n","\n","    writer.close()\n","\n","if __name__ == '__main__':\n","    warnings.filterwarnings(\"ignore\")\n","    config = get_config()\n","    train(config)\n"]},{"cell_type":"markdown","metadata":{"id":"ARfq3b1bL7eI"},"source":["###Tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m3FPwYqsL7MP"},"outputs":[],"source":["%reload_ext tensorboard\n","\n","import tensorflow as tf\n","import tensorboard\n","\n","log_dir = \"/content/drive/MyDrive/Colab Notebooks/T-CLM/T-CLM/\"\n","%tensorboard --logdir {log_dir}"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}