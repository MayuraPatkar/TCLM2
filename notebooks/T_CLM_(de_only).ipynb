{"cells":[{"cell_type":"markdown","metadata":{"id":"Sc2SvHlgLP_J"},"source":["###Connect to Drive"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rk5VitodxQv_","outputId":"518d36e1-481b-483f-8a5b-f3e6f7f50ef2","executionInfo":{"status":"ok","timestamp":1726665426832,"user_tz":-330,"elapsed":22103,"user":{"displayName":"Mayura","userId":"14003289813185148827"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"BgrYC2g_xDBJ"},"source":["###configuration file"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"djwQ4krXwH0M","executionInfo":{"status":"ok","timestamp":1726665931805,"user_tz":-330,"elapsed":510,"user":{"displayName":"Mayura","userId":"14003289813185148827"}}},"outputs":[],"source":["def get_config():\n","    return {\n","        \"model\":\"T-CLM\",\n","        \"logs\": \"/content/drive/MyDrive/Colab Notebooks/T-CLM_log/T-CLM\",\n","        \"batch_size\": 8,\n","        \"num_epochs\": 30,\n","        \"lr\": 1e-4,\n","        \"seq_len\": 512,\n","        \"d_model\": 768,\n","        \"n_layers\": 12,\n","        \"head\": 12,\n","        \"d_ff\": 3072,\n","        \"dropout\": 0.1,\n","        \"masking_prob\": 0.15,\n","        \"vocab_size\": 50261,\n","        \"model_file_path\": \"/content/drive/MyDrive/Colab Notebooks/T-CLM/T-CLM.pt\",\n","        \"tokenizer_file\": \"/content/drive/MyDrive/Colab Notebooks/T-CLM/tokenizer.json\",\n","    }\n"]},{"cell_type":"markdown","metadata":{"id":"g2a7fTMsxGUH"},"source":["###BPE Tokenizer\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"yv9S_PQcwj0i","executionInfo":{"status":"ok","timestamp":1726665426834,"user_tz":-330,"elapsed":13,"user":{"displayName":"Mayura","userId":"14003289813185148827"}}},"outputs":[],"source":["from pathlib import Path\n","from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n","\n","def get_all_sentences(ds, field):\n","    for item in ds:\n","        yield item[field]\n","\n","def build_or_get_tokenizer(config, ds):\n","    tokenizer_path = Path(config['tokenizer_file'])\n","    if not tokenizer_path.exists():\n","        tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n","        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n","        trainer = trainers.BpeTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\", \"[MASK]\"], min_frequency=1)\n","        tokenizer.train_from_iterator(get_all_sentences(ds, \"text\"), trainer=trainer)\n","        tokenizer.save(str(tokenizer_path))\n","    else:\n","        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n","\n","    return tokenizer"]},{"cell_type":"markdown","metadata":{"id":"o1LIieN_LbB5"},"source":["###Data Pipeline"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"1VPhoImUwpcQ","executionInfo":{"status":"ok","timestamp":1726665432686,"user_tz":-330,"elapsed":5864,"user":{"displayName":"Mayura","userId":"14003289813185148827"}}},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader, random_split\n","import json\n","from torch.utils.data import Dataset\n","\n","class BilingualDataset(Dataset):\n","    def __init__(self, ds, tokenizer, seq_len):\n","        self.seq_len = seq_len\n","        self.ds = ds\n","        self.tokenizer = tokenizer\n","        self.sos_token = torch.tensor([tokenizer.token_to_id(\"[SOS]\")], dtype=torch.int64)\n","        self.eos_token = torch.tensor([tokenizer.token_to_id(\"[EOS]\")], dtype=torch.int64)\n","        self.pad_token = torch.tensor([tokenizer.token_to_id(\"[PAD]\")], dtype=torch.int64)\n","\n","    def __len__(self):\n","        return len(self.ds)\n","\n","    def __getitem__(self, idx):\n","        text = self.ds[idx]['text']\n","        input_tokens = self.tokenizer.encode(text).ids\n","\n","        # Truncate if too long\n","        if len(input_tokens) > self.seq_len - 2:\n","            input_tokens = input_tokens[:self.seq_len - 2]\n","\n","        num_padding_tokens = self.seq_len - len(input_tokens) - 2\n","\n","        input = torch.cat(\n","            [\n","                self.sos_token,\n","                torch.tensor(input_tokens, dtype=torch.int64),\n","                self.eos_token,\n","                torch.tensor([self.pad_token] * num_padding_tokens, dtype=torch.int64),\n","            ],\n","            dim=0,\n","        )\n","\n","        # The label is shifted right by one\n","        label = torch.cat(\n","            [\n","                torch.tensor(input_tokens, dtype=torch.int64),\n","                self.eos_token,\n","                torch.tensor([self.pad_token] * (num_padding_tokens + 1), dtype=torch.int64),\n","            ],\n","            dim=0,\n","        )\n","\n","        assert input.size(0) == self.seq_len\n","        assert label.size(0) == self.seq_len\n","\n","        return {\n","            \"input\": input,\n","            \"label\": label,\n","            \"mask\": (input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n","            \"text\": text,\n","        }\n"]},{"cell_type":"markdown","metadata":{"id":"oFjxO_cGMVg5"},"source":["###Load dataset"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"MQyv27OIMG5J","executionInfo":{"status":"ok","timestamp":1726665432687,"user_tz":-330,"elapsed":11,"user":{"displayName":"Mayura","userId":"14003289813185148827"}}},"outputs":[],"source":["def get_ds(config):\n","    with open('/content/drive/MyDrive/Colab Notebooks/T-CLM/dataset.json', 'r', encoding='utf-8') as f:\n","        ds_raw = json.load(f)\n","\n","    # ds_raw = load_dataset(f\"bookcorpus/bookcorpus\", f\"plain_text\", split='train', trust_remote_code=True)\n","\n","    tokenizer = build_or_get_tokenizer(config, ds_raw)\n","    train_ds_size = int(0.95 * len(ds_raw))\n","    val_ds_size = len(ds_raw) - train_ds_size\n","    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n","\n","    train_ds = BilingualDataset(train_ds_raw, tokenizer, config['seq_len'])\n","    val_ds = BilingualDataset(val_ds_raw, tokenizer, config['seq_len'])\n","\n","    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n","    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n","\n","    return train_dataloader, val_dataloader, tokenizer"]},{"cell_type":"code","source":["config = get_config()\n","train_dataloader, val_dataloader, tokenizer = get_ds(config)\n","\n","# for batch in train_dataloader:\n","#   print(batch['input'][0])\n","#   print(batch['label'][0])\n","#   break\n","\n","print(tokenizer.get_vocab_size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oz9FSAoxR_Zu","executionInfo":{"status":"ok","timestamp":1726665825948,"user_tz":-330,"elapsed":419,"user":{"displayName":"Mayura","userId":"14003289813185148827"}},"outputId":"94d28654-04fc-4466-e90a-591e7828c315"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["50261\n"]}]},{"cell_type":"markdown","metadata":{"id":"y1f8UxP8MRnU"},"source":["###Casual masking"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"5EMOgLQOMMDT","executionInfo":{"status":"ok","timestamp":1726665667236,"user_tz":-330,"elapsed":428,"user":{"displayName":"Mayura","userId":"14003289813185148827"}}},"outputs":[],"source":["def causal_mask(size):\n","    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n","    return mask == 0"]},{"cell_type":"markdown","metadata":{"id":"QI2RCCppxLNv"},"source":["###Transformer model"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"ycm0_YdhwnJL","executionInfo":{"status":"ok","timestamp":1726665669463,"user_tz":-330,"elapsed":414,"user":{"displayName":"Mayura","userId":"14003289813185148827"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import math\n","\n","class LayerNormalization(nn.Module):\n","    def __init__(self, features: int, eps: float = 1e-6) -> None:\n","        super().__init__()\n","        self.eps = eps\n","        self.alpha = nn.Parameter(torch.ones(features))\n","        self.bias = nn.Parameter(torch.zeros(features))\n","\n","    def forward(self, x):\n","        mean = x.mean(dim=-1, keepdim=True)\n","        std = x.std(dim=-1, keepdim=True)\n","        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n","\n","class FeedForwardBlock(nn.Module):\n","    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n","        super().__init__()\n","        self.linear_1 = nn.Linear(d_model, d_ff)\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear_2 = nn.Linear(d_ff, d_model)\n","\n","    def forward(self, x):\n","        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n","\n","class InputEmbeddings(nn.Module):\n","    def __init__(self, d_model: int, vocab_size: int) -> None:\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, d_model)\n","\n","    def forward(self, x):\n","        return self.embedding(x) * math.sqrt(self.embedding.embedding_dim)\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n","        super().__init__()\n","        self.dropout = nn.Dropout(dropout)\n","        pe = torch.zeros(seq_len, d_model)\n","        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:, :x.shape[1], :].requires_grad_(False)\n","        return self.dropout(x)\n","\n","class ResidualConnection(nn.Module):\n","    def __init__(self, features: int, dropout: float) -> None:\n","        super().__init__()\n","        self.dropout = nn.Dropout(dropout)\n","        self.norm = LayerNormalization(features)\n","\n","    def forward(self, x, sublayer):\n","        return x + self.dropout(sublayer(self.norm(x)))\n","\n","class MultiHeadAttentionBlock(nn.Module):\n","\n","    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n","        super().__init__()\n","        self.d_model = d_model\n","        self.h = h\n","        assert d_model % h == 0, \"d_model is not divisible by h\"\n","\n","        self.d_k = d_model // h\n","        self.w_q = nn.Linear(d_model, d_model, bias=False)\n","        self.w_k = nn.Linear(d_model, d_model, bias=False)\n","        self.w_v = nn.Linear(d_model, d_model, bias=False)\n","        self.w_o = nn.Linear(d_model, d_model, bias=False)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    @staticmethod\n","    def attention(query, key, value, mask, dropout: nn.Dropout):\n","        d_k = query.shape[-1]\n","        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n","        if mask is not None:\n","            attention_scores.masked_fill_(mask == 0, -1e9)\n","        attention_scores = attention_scores.softmax(dim=-1)\n","        if dropout is not None:\n","            attention_scores = dropout(attention_scores)\n","        return (attention_scores @ value), attention_scores\n","\n","    def forward(self, q, k, v, mask):\n","        query = self.w_q(q)\n","        key = self.w_k(k)\n","        value = self.w_v(v)\n","\n","        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n","        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n","        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n","\n","        # Calculate attention\n","        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n","\n","        # Combine all the heads together\n","        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n","\n","        # Multiply by Wo\n","        return self.w_o(x)\n","\n","\n","class DecoderBlock(nn.Module):\n","    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n","        super().__init__()\n","        self.self_attention_block = self_attention_block\n","        self.feed_forward_block = feed_forward_block\n","        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n","\n","    def forward(self, x, mask):\n","        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, mask))\n","        x = self.residual_connections[1](x, self.feed_forward_block)\n","        return x\n","\n","class Decoder(nn.Module):\n","    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n","        super().__init__()\n","        self.layers = layers\n","        self.norm = LayerNormalization(features)\n","\n","    def forward(self, x, mask):\n","        for layer in self.layers:\n","            x = layer(x, mask)\n","        return self.norm(x)\n","\n","class ProjectionLayer(nn.Module):\n","    def __init__(self, d_model, vocab_size) -> None:\n","        super().__init__()\n","        self.proj = nn.Linear(d_model, vocab_size)\n","\n","    def forward(self, x) -> None:\n","        return self.proj(x)\n","\n","class Transformer(nn.Module):\n","    def __init__(self, decoder: Decoder, tgt_embed: InputEmbeddings, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n","        super().__init__()\n","        self.decoder = decoder\n","        self.tgt_embed = tgt_embed\n","        self.tgt_pos = tgt_pos\n","        self.projection_layer = projection_layer\n","\n","    def decode(self, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n","        tgt = self.tgt_embed(tgt)\n","        tgt = self.tgt_pos(tgt)\n","        return self.decoder(tgt, tgt_mask)\n","\n","    def project(self, x):\n","        return self.projection_layer(x)\n","\n","def build_transformer(tgt_vocab_size: int, tgt_seq_len: int, d_model: int, N: int, h: int, dropout: float, d_ff: int) -> Transformer:\n","    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n","    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n","\n","    decoder_blocks = []\n","    for _ in range(N):\n","        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n","        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n","        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, feed_forward_block, dropout)\n","        decoder_blocks.append(decoder_block)\n","\n","    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n","    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n","\n","    transformer = Transformer(decoder, tgt_embed, tgt_pos, projection_layer)\n","\n","    for p in transformer.parameters():\n","        if p.dim() > 1:\n","            nn.init.xavier_uniform_(p)\n","\n","    return transformer\n"]},{"cell_type":"code","source":["config = get_config()\n","\n","model = build_transformer(config['vocab_size'], config['seq_len'], config['d_model'], config['n_layers'], config['head'], config['dropout'], config['d_ff'])\n","print(model)\n","\n","# for name, param in model.named_parameters():\n","#     print(name, param)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T-1nxa2dW0Iy","executionInfo":{"status":"ok","timestamp":1726666825619,"user_tz":-330,"elapsed":4687,"user":{"displayName":"Mayura","userId":"14003289813185148827"}},"outputId":"76280c2a-cd2d-4b51-f4d4-d59d8b63b306"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Transformer(\n","  (decoder): Decoder(\n","    (layers): ModuleList(\n","      (0-11): 12 x DecoderBlock(\n","        (self_attention_block): MultiHeadAttentionBlock(\n","          (w_q): Linear(in_features=768, out_features=768, bias=False)\n","          (w_k): Linear(in_features=768, out_features=768, bias=False)\n","          (w_v): Linear(in_features=768, out_features=768, bias=False)\n","          (w_o): Linear(in_features=768, out_features=768, bias=False)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward_block): FeedForwardBlock(\n","          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (residual_connections): ModuleList(\n","          (0-1): 2 x ResidualConnection(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (norm): LayerNormalization()\n","          )\n","        )\n","      )\n","    )\n","    (norm): LayerNormalization()\n","  )\n","  (tgt_embed): InputEmbeddings(\n","    (embedding): Embedding(50261, 768)\n","  )\n","  (tgt_pos): PositionalEncoding(\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (projection_layer): ProjectionLayer(\n","    (proj): Linear(in_features=768, out_features=50261, bias=True)\n","  )\n",")\n"]}]},{"cell_type":"code","source":["config = get_config()\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","model = build_transformer(config['vocab_size'], config['seq_len'], config['d_model'], config['n_layers'], config['head'], config['dropout'], config['d_ff'])\n","\n","train_dataloader, _, _ = get_ds(config)\n","\n","for batch in train_dataloader:\n","    input_ids = batch['input'][0].to(device)\n","    labels = batch['label'][0].to(device)\n","    break\n","\n","print(\"Input shape: \",input_ids.shape)\n","\n","mask = torch.triu(torch.ones(config['seq_len'], config['seq_len']), diagonal=1)  # Upper triangular matrix\n","# print(mask)\n","mask = mask.unsqueeze(0)\n","\n","print(\"Mask shape: \", mask.shape)\n","\n","with torch.no_grad():  # No need to calculate gradients\n","    output = model.decode(input_ids, mask)\n","\n","print(\"Output shape: \",output.shape)\n","\n","# Project the output to the vocabulary size\n","logits = model.project(output)\n","\n","print(\"Logits.shape: \",logits.shape)\n","\n","loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n","loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n","print(\"loss: \",loss.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Iadt_YCaS1mV","executionInfo":{"status":"ok","timestamp":1726666589055,"user_tz":-330,"elapsed":7482,"user":{"displayName":"Mayura","userId":"14003289813185148827"}},"outputId":"23379d58-5771-443e-e7e8-f94d58dba84b"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Input shape:  torch.Size([512])\n","Mask shape:  torch.Size([1, 512, 512])\n","Output shape:  torch.Size([1, 512, 768])\n","Logits.shape:  torch.Size([1, 512, 50261])\n","loss:  10.775079727172852\n"]}]},{"cell_type":"markdown","metadata":{"id":"WjUFRoFrLCDz"},"source":["###Validation run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gpRG2ZWdKq3U"},"outputs":[],"source":["def run_validation(model, val_dataloader, tokenizer, seq_len, device, log_fn, global_step, writer):\n","    model.eval()\n","    total_loss = 0.0\n","    num_batches = len(val_dataloader)\n","    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n","\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids = batch['input'].to(device)\n","            labels = batch['label'].to(device)\n","\n","            # Create causal mask\n","            seq_len = input_ids.size(1)\n","            causal_mask = torch.tril(torch.ones((seq_len, seq_len), device=device)).unsqueeze(0).unsqueeze(0)\n","\n","            logits = model.decode(input_ids, causal_mask)\n","            logits = model.project(logits)\n","\n","            logits = logits.view(-1, logits.size(-1))\n","            labels = labels.view(-1)\n","\n","            loss = loss_fn(logits, labels)\n","            total_loss += loss.item()\n","\n","    avg_loss = total_loss / num_batches\n","    perplexity = math.exp(avg_loss)\n","    print(f\"Validation | Avg Loss: {avg_loss:.4f} | Perplexity: {perplexity:.2f}\")\n","\n","    writer.add_scalar('Validation/Loss', avg_loss, global_step)\n","    writer.add_scalar('Validation/Perplexity', perplexity, global_step)\n","\n","    model.train()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FxGdDwBBKuwj"},"outputs":[],"source":["def get_weights_file_path(config):\n","    model_file_path = config.get('model_file_path', '')\n","    if Path(model_file_path).exists():\n","        return str(model_file_path)\n","    else:\n","        return None"]},{"cell_type":"markdown","metadata":{"id":"oFZOdFDDK7QL"},"source":["###Get model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h2w9W2hpKxQD"},"outputs":[],"source":["def get_decoder_only_model(config, vocab_size):\n","    model = build_transformer(vocab_size, config['seq_len'], config['d_model'], config['n_layers'], config['head'], config['dropout'], config['d_ff'])\n","    return model\n"]},{"cell_type":"markdown","metadata":{"id":"K6Z-sKhGvQSc"},"source":["###Preload model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sjl15GUkvS8m"},"outputs":[],"source":["def load_model(config, device, tokenizer, optimizer):\n","    if os.path.exists(config['model_file_path']):\n","        checkpoint = torch.load(config['model_file_path'], map_location=device)\n","        model = get_decoder_only_model(config, tokenizer.get_vocab_size()).to(device)\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        epoch = checkpoint['epoch']\n","        global_step = checkpoint.get('global_step', 0)\n","        print(f\"Loaded checkpoint from epoch {epoch}, global_step {global_step}\")\n","        return model, epoch, global_step\n","    else:\n","        print(\"No checkpoint found. Starting training from scratch.\")\n","        model = get_decoder_only_model(config, tokenizer.get_vocab_size()).to(device)\n","        return model, 0, 0\n"]},{"cell_type":"markdown","metadata":{"id":"sPOUVLnjKz2-"},"source":["###Training loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XzdqabrRx8GQ"},"outputs":[],"source":["!pip install torchmetrics"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":200},"id":"Wmm5oPx4wr3U","outputId":"6c99a95d-be08-4154-b163-0ea080028bcc","executionInfo":{"status":"error","timestamp":1726640692890,"user_tz":-330,"elapsed":12072,"user":{"displayName":"Mayura","userId":"14003289813185148827"}}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'get_config' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-edbee41c324a>\u001b[0m in \u001b[0;36m<cell line: 93>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'get_config' is not defined"]}],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, random_split\n","import warnings\n","import os\n","import sys\n","import json\n","from pathlib import Path\n","from tqdm import tqdm\n","from torch.utils.tensorboard import SummaryWriter\n","import torchmetrics\n","\n","def train(config):\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    print(\"Using device:\", device)\n","    if device == 'cuda':\n","        print(f\"Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n","        print(f\"Device memory: {torch.cuda.get_device_properties(device).total_memory / 1024 ** 3:.2f} GB\")\n","    device = torch.device(device)\n","\n","    train_dataloader, val_dataloader, tokenizer = get_ds(config)\n","    model = get_decoder_only_model(config, tokenizer.get_vocab_size()).to(device)\n","    writer = SummaryWriter(config['logs'])\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], eps=1e-8)\n","    model, initial_epoch, global_step = load_model(config, device, tokenizer, optimizer)\n","    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n","\n","    # Initialize the learning rate scheduler\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","    for epoch in range(initial_epoch, config['num_epochs']):\n","        torch.cuda.empty_cache()\n","        model.train()\n","        batch_iterator = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{config['num_epochs']}\")\n","        total_loss = 0.0\n","        num_batches = len(train_dataloader)\n","\n","        for batch in batch_iterator:\n","            input_ids = batch['input'].to(device)  # (batch_size, seq_len)\n","            labels = batch['label'].to(device)        # (batch_size, seq_len)\n","\n","            seq_len = input_ids.size(1)   # Shape: (1, 1, seq_len, seq_len)\n","            causal_mask = torch.tril(torch.ones((seq_len, seq_len), device=device)).unsqueeze(0).unsqueeze(0)\n","\n","            optimizer.zero_grad(set_to_none=True)\n","\n","            # Forward pass through the decoder-only model\n","            logits = model.decode(input_ids, causal_mask)  # (batch_size, seq_len, vocab_size)\n","            logits = model.project(logits)                 # (batch_size, seq_len, vocab_size)\n","\n","            # Reshape logits and labels for computing loss\n","            logits = logits.view(-1, logits.size(-1))      # (batch_size * seq_len, vocab_size)\n","            labels = labels.view(-1)                       # (batch_size * seq_len)\n","\n","            loss = loss_fn(logits, labels)\n","\n","            loss.backward()\n","\n","\n","            # Optional: Gradient clipping for stability\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","            optimizer.step()\n","            scheduler.step()\n","\n","            total_loss += loss.item()\n","            global_step += 1\n","            writer.add_scalar('Train/Loss', loss.item(), global_step)\n","            batch_iterator.set_postfix({'Loss': loss.item()})\n","\n","        # End of epoch logging\n","        avg_loss = total_loss / num_batches\n","        writer.add_scalar('Train/Average_Loss', avg_loss, epoch + 1)\n","\n","        # Validation\n","        run_validation(model, val_dataloader, tokenizer, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n","\n","        scheduler.step()\n","\n","        # Save the model checkpoint\n","        torch.save({\n","            'epoch': epoch + 1,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'scheduler_state_dict': scheduler.state_dict(),\n","            'global_step': global_step,\n","        }, config['model_file_path'])\n","\n","    writer.close()\n","\n","if __name__ == '__main__':\n","    warnings.filterwarnings(\"ignore\")\n","    config = get_config()\n","    train(config)\n"]},{"cell_type":"markdown","metadata":{"id":"ARfq3b1bL7eI"},"source":["###Tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m3FPwYqsL7MP"},"outputs":[],"source":["%reload_ext tensorboard\n","\n","import tensorflow as tf\n","import tensorboard\n","\n","log_dir = \"/content/drive/MyDrive/Colab Notebooks/T-CLM/T-CLM/\"\n","%tensorboard --logdir {log_dir}"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}